syntax = "proto3";

package exa.model_management_pb;

option go_package = "github.com/Exafunction/Exafunction/exa/model_management_pb";

import "buf/validate/validate.proto";
import "exa/trainer_pb/config.proto";

service ModelManagementService {
  rpc AddModel (.exa.model_management_pb.AddModelRequest) returns (.exa.model_management_pb.AddModelResponse) {}
  rpc DeleteModel (.exa.model_management_pb.DeleteModelRequest) returns (.exa.model_management_pb.DeleteModelResponse) {}
  rpc ListModels (.exa.model_management_pb.ListModelsRequest) returns (.exa.model_management_pb.ListModelsResponse) {}
  rpc StartInferenceServer (.exa.model_management_pb.StartInferenceServerRequest) returns (.exa.model_management_pb.StartInferenceServerResponse) {}
  rpc StopInferenceServer (.exa.model_management_pb.StopInferenceServerRequest) returns (.exa.model_management_pb.StopInferenceServerResponse) {}
  rpc UpdateInferenceServer (.exa.model_management_pb.UpdateInferenceServerRequest) returns (.exa.model_management_pb.UpdateInferenceServerResponse) {}
  rpc ListInferenceServers (.exa.model_management_pb.ListInferenceServersRequest) returns (.exa.model_management_pb.ListInferenceServersResponse) {}
  rpc PollInferenceServer (.exa.model_management_pb.PollInferenceServerRequest) returns (.exa.model_management_pb.PollInferenceServerResponse) {}
}

message AddModelRequest {
  string model_name = 1;
  bool overwrite = 3;
  bool async = 5;
  oneof model_path {
    string model_gcs_path = 2;
    string model_local_path = 4;
  }
}

message AddModelResponse {
}

message DeleteModelRequest {
  string model_name = 1;
}

message DeleteModelResponse {
}

message ModelInPV {
  string model_name = 1;
  int64 model_size = 2;
}

message ListModelsRequest {
}

message ListModelsResponse {
  repeated .exa.model_management_pb.ModelInPV models = 1;
}

message InferenceServerEnvVariable {
  string name = 1;
  string value = 2;
}

message StartInferenceServerRequest {
  int32 port = 2;
  int32 num_gpus = 3;
  int32 num_cpus = 4;
  int32 memory_gb = 5;
  string gpu_type = 22;
  int32 model_parallelism = 6;
  int32 gamma = 11;
  float draft_temperature = 12;
  int32 prompt_cache_cpu_memory_mib = 19;
  int32 replicas = 8;
  string suffix = 7;
  repeated string custom_flags = 9;
  bool run_async = 10;
  string inference_server_image_path = 13;
  bool blocking = 15;
  int32 max_generation_sequences = 16;
  int32 server_version = 17;
  string deployment_name_override = 18;
  int32 speculative_copy_length = 21;
  repeated .exa.model_management_pb.InferenceServerEnvVariable inference_server_env = 20;
  oneof model {
    string model_name = 1;
    .exa.model_management_pb.InferenceModels models = 14;
  }
  optional string deployment_yaml = 23;
}

message StartInferenceServerResponse {
  reserved "inference_server_service_ip_address";
  reserved 1;
  string service_domain = 2;
}

message StopInferenceServerRequest {
  reserved "inference_server_service_ip_address";
  reserved 1;
  string service_domain = 2;
}

message StopInferenceServerResponse {
}

message UpdateInferenceServerRequest {
  reserved "inference_server_service_ip_address";
  reserved 1;
  int32 replicas = 3;
  int32 num_gpus = 4;
  int32 num_cpus = 5;
  int32 memory_gb = 6;
  int32 model_parallelism = 7;
  int32 gamma = 10;
  float draft_temperature = 11;
  int32 max_generation_sequences = 13;
  repeated string custom_flags = 9;
  oneof inference_server_id {
    string service_domain = 14;
    string deployment_name = 2;
  }
  optional bool blocking = 12;
}

message UpdateInferenceServerResponse {
}

message ListInferenceServersRequest {
}

message ListInferenceServersResponse {
  repeated .exa.model_management_pb.InferenceServerData inference_servers = 1;
}

message InferenceServerData {
  reserved "ip_address";
  reserved 1;
  message KubernetesLabelsEntry {
    string key = 1;
    string value = 2;
  }

  string service_domain = 18;
  int32 port = 2;
  string service_name = 3;
  repeated string model_names = 4;
  string age = 13;
  int32 ready_replicas = 12;
  int32 replicas = 5;
  int32 num_gpus = 6;
  int32 num_cpus = 7;
  int32 memory_gb = 8;
  int32 model_parallelism = 9;
  int32 gamma = 14;
  float draft_temperature = 15;
  repeated string custom_flags = 10;
  repeated .exa.model_management_pb.InferenceServerData.KubernetesLabelsEntry kubernetes_labels = 11;
  bool blocking = 16;
  int32 max_generation_sequences = 17;
  int32 data_parallelism = 19;
}

message InferenceModel {
  string model_name = 1;
  bool overwrite = 5;
  bool draft = 2;
  .exa.model_management_pb.InferenceRole role = 3;
  .exa.trainer_pb.InferenceConfig custom_inference_config = 15;
  optional string model_path = 4;
}

message InferenceModels {
  repeated .exa.model_management_pb.InferenceModel models = 1;
}

message PollInferenceServerRequest {
  string service_domain = 1;
}

message PollInferenceServerResponse {
  string service_domain = 1;
  int32 port = 2;
  int32 ready_replicas = 3;
  int32 total_replicas = 4;
  bool ready = 5;
  repeated string pod_ips = 6;
}

enum InferenceRole {
  INFERENCE_ROLE_UNSPECIFIED = 0;
  INFERENCE_ROLE_CHAT = 1;
  INFERENCE_ROLE_AUTOCOMPLETE = 2;
  INFERENCE_ROLE_EMBEDDING = 3;
}

